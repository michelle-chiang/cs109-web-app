<!DOCTYPE html>
<html lang="en">
<head>
    <header style="margin-top: -40px;">
    <header style="margin-left: 25px;">
    <header style="margin-right: 25px;">
    <meta charset="UTF-8">
    <title>CS109 Final Project</title>
    <link href="//maxcdn.bootstrapcdn.com/bootswatch/3.2.0/simplex/bootstrap.min.css" rel="stylesheet">
    <link rel="stylesheet" href="css/style.css">
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js"></script>
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/2.1.1/jquery.min.js"></script>
    <h1>CS109a Final Project: Spotify Playlist Analysis</h1>
    <h2>Group 31: Michelle Chiang, David Seong, Emily Chen</h2>
    <h2>December 7, 2017</h2>
</head>
<body>

<ul class="nav nav-tabs" id="my-tabs">
  <li class=""><a href="/" data-toggle="tab" aria-expanded="false">Introduction</a></li>
  <li class=""><a href="/EDA" data-toggle="tab">EDA</a></li>
  <li class=""><a href="/model" data-toggle="tab" aria-expanded="false">Model</a></li>
  <li class="active"><a href="/results" data-toggle="tab" aria-expanded="true">Results</a></li>
</ul>
<div id="myTabContent" class="tab-content">
  <h2><b>Results and Conclusions</b></h2>
	<p>For our first step (Spotify predictors-only), both our polynomial term regression and the random forest regression produced R^2 values that beat our baseline linear regression value of 0.181 so we decided to fine tune both of these models. We first tried fine tuning the polynomial term regression using both lasso and ridge. The ridge regression produced a R^2 value of 0.263 and the lasso produced a R^2 value of 0.268. We then fine tuned the random forest regression by first setting the depth to the optimal depth we found in step one (depth = 9), which produced a R^2 value of 0.290, and optimized the number of trees. We tried 2, 4, 8, 16, 32, 64, 128, and 256 trees and found that 32 trees was the optimal number and produced a R^2 value of 0.33. We then optimized for the best number of predictors and found that 4 was the optimal number with an R^2 of 0.32. Thus our best model in step 1 was our optimized random forest model that produced a R^2 of 0.32.</p>
  <p>For our second step, we followed the same strategy (shotgun, fine tuning) but with an increased dataset. This new dataset had more predictors such as energy, danceability, time signature, and key signature. Again, the two best models were the polynomial term regression and random forest. Although both as is beat the baseline model, we decided to fine tune them anyway to see if we could further improve them. To do so, we performed lasso and ridge on the polynomial term regression which produced values of 0.222 and 0.256, respectively. Both of these values beat the baseline of 0.181 and were an improvement over simple polynomial term regression which had produced a R^2 value of 0.206. We then fine tuned the random forest model using the same strategy (optimizing depth, number of trees, number of predictors). The best depth seemed to be 9 again and produced a value of 0.240. The best number of trees seemed to be 32 and produced a R^2 of 0.302 which was an improvement. The best number of predictors seemed to be 25 which produced a final R^2 value of .2995 which was essentially the same as the one before. Thus, our best model in step 2 was our optimized random forest that produced a R^2 value of ~0.3.</p>
  <p>One shortcoming that we observed was that our random forest using Spotify predictors-only dataset was actually (slightly) better than our complete dataset. This is certainly possible if the predictors we added were not very helpful and/or collinear with previous predictors. But, if given more time this is one area where we would further investigate by examining random forest regressions with both polynomial and interaction terms as well as testing for collinearity. That said, a strength of our project is that we tried various models in our shotgun approach before selecting the best ones to further pursue. We did not incorporate a stepwise method because lasso performs variable selection automatically and is much more efficient than stepwise. In addition, our regularization/optimization for the random forest model attempted to cover the major characteristics of random forest that we learned about in class (depth, tree number, predictor number) and despite the small decrease from step one to step two, did improve the model within each step. </p>
  <h3>Recommendation System</h3>
  <p>hello</p>

</div>


</body>
</html>