<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>CS109 Final Project</title>
    <link href="//maxcdn.bootstrapcdn.com/bootswatch/3.2.0/simplex/bootstrap.min.css" rel="stylesheet">
    <link rel="stylesheet" href="css/style.css">
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js"></script>
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/2.1.1/jquery.min.js"></script>
</head>
<body>
<% include /partials/header %>

<ul class="nav nav-tabs" id="my-tabs">
  <li class="active"><a href="/" data-toggle="tab" aria-expanded="true">Introduction</a></li>
  <li class=""><a href="/EDA" data-toggle="tab">EDA</a></li>
  <li class=""><a href="/model" data-toggle="tab" aria-expanded="false">Model</a></li>
  <li class=""><a href="/results" data-toggle="tab" aria-expanded="false">Results</a></li>
</ul>

<div class="jumbotron">
  <h2><b>Problem Statement and Motivation</b></h2>
  <p>The purpose of this project is to build a model to predict the popularity (number of followers) of a Spotify playlist given different features of the playlist. We scraped, cleaned, and combined data from various web sources to generate a model that generates a predicted value for this number.</p>
</div>

<div id="intro-content" class="tab-content">
  <h2><b>Introduction and Description of Data</b></h2>
  <p>Music is such an important of our lives. It connects people and allows for the flourishing of creativity. Part of the enjoyment of music is being able to discover new music as well as share it with other people. Spotify seeks to do this through the development of its music recommendation system, a valuable asset in attracting more customers and recommending good music for its users. One of Spotify’s main functions is generating playlists, which it can then recommend to its users with the goal of providing them with collections of music they might enjoy.</p>
  <p>Playlists are composed of a number of individual tracks, and Spotify has generated many of them already. To obtain the data we used for this project, we scraped data from <a href="https://developer.spotify.com/web-api/">Spotify’s Web API</a>, namely all of the Spotify-owned playlists and their features. In addition, we also scraped data for each individual track in each of these playlists. Then, in order to obtain an even richer set of data on which to build our model, we also scraped data regarding each of the artists and song albums connected to the songs we scraped. After parsing through and cleaning all the data we scraped, the dataset we used to train and build our model contains 1,642 playlists (we scraped 50,000+ songs from Spotify to help generate this dataset).</p>
  <p>Our final Spotify dataset has the following features, with our model ideally predicting the ‘followers’ column (additional features scraped from the <a href="https://labrosa.ee.columbia.edu/millionsong/">Million Song Dataset</a> are listed at the end):</p>

  <h4>Spotify Features</h4>
  <table class="table table-striped table-hover">
    <thead>
      <tr>
        <th>attribute</th>
        <th>description</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>followers</td>
        <td>number of followers on Spotify</td>
      </tr>
      <tr>
        <td>name</td>
        <td>name of playlist</td>
      </tr>
      <tr>
        <td>owner</td>
        <td>owner of playlist (all are Spotify)</td>
      </tr>
      <tr>
        <td>track_ids</td>
        <td>list of Spotify ID’s of all tracks in playlist</td>
      </tr>
      <tr>
        <td>num_tracks</td>
        <td>number of tracks in playlist</td>
      </tr>
      <tr>
        <td>avg_song_popularity</td>
        <td>average popularity of component tracks (value between 0 and 100, with 100 being the most popular)</td>
      </tr>
      <tr>
        <td>avg_loudness</td>
        <td>average loudness of component tracks (decibels)</td>
      </tr>
      <tr>
        <td>avg_speechiness</td>
        <td>average speechiness of component tracks (value  between 0.0 and 1.0, with 1.0 being the most speech-like); measures the  presence of spoken words in a track (values above 0.66 represent tracks that are entirely speech, between 0.33 and 0.66 represents a mix of music and  speech, below 0.33 represents entirely music)</td>
      </tr>
      <tr>
        <td>avg_acousticness</td>
        <td>average confidence level for whether or not  component tracks are acoustic (value between 0.0 and 1.0, with 1.0 being the  highest confidence that track is acoustic)</td>
      </tr>
      <tr>
        <td>avg_instrumentalness</td>
        <td>average value of how instrumental, as opposed to vocal, component tracks are(value between 0.0 and 1.0, with 1.0 being  entirely instrumental)</td>
      </tr>
      <tr>
        <td>avg_liveness</td>
        <td>average value for probability that component  tracks were performed live (value between 0.0 and 1.0, with 1.0 representing  high likelihood that a track was performed live)</td>
      </tr>
      <tr>
        <td>avg_valence</td>
        <td>average musical positiveness of component tracks (value between 0.0 and 1.0, with 1.0 being the most positive); high valence  tracks sound more positive (e.g. happy, cheerful, euphoric) and low valence sound more negative (e.g. sad, depressed, angry)</td>
      </tr>
      <tr>
        <td>avg_num_artists</td>
        <td>average number of artists that performed in  component tracks</td>
      </tr>
      <tr>
        <td>avg_num_markets</td>
        <td>average number of countries in which component tracks can be played</td>
      </tr>
      <tr>
        <td>majority_explicit</td>
        <td>binary value indicating whether the majority of  component tracks contain explicit lyrics (1 = explicit, 0 = not explicit)</td>
      </tr>
      <tr>
        <td>majoirty_mode</td>
        <td>binary value indicating the modality of the majority of component tracks (1 = major, 0 = minor)</td>
      </tr>
      <tr>
        <td>majority_album_type</td>
        <td>type of album from which the majority of component tracks come from (album,  compilation, single, NaN)</td>
      </tr>
      <tr>
        <td>avg_album_popularity</td>
        <td>average popularity of albums on which component  tracks were released (value between 0.0 and 100.0, with 100.0 being the most popular)</td>
      </tr>
      <tr>
        <td>avg_album_release_year</td>
        <td>average release year of albums on which component tracks were released</td>
      </tr>
      <tr>
        <td>avg_artist_popularity</td>
        <td>average popularity of artists of component track (value between 0.0 and 100.0, with 100.0 being the most popular)</td>
      </tr>
      <tr>
        <td>avg_artist_followers</td>
        <td>average number of followers of artists of component tracks</td>
      </tr>
      <tr>
        <td>majoirty_artist_genres</td>
        <td>genre of majority of artists of component tracks</td>
      </tr>
    </tbody>
  </table> 

  <h4>Additional Features</h4>
  <table class="table table-striped table-hover">
    <thead>
      <tr>
        <th>attribute</th>
        <th>description</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>avg_danceability</td>
        <td>average danceability of component tracks (value between 0.0 and 1.0, with 1.0 being the most danceable); measure of tracks’ suitability for dancing based on a combination of musical elements, including tempo, rhythm stability, beat strength, and overall regularity</td>
      </tr>
      <tr>
        <td>avg_duration_ms</td>
        <td>average duration of component tracks (milliseconds)</td>
      </tr>
      <tr>
        <td>avg_energy</td>
        <td>average energy of component tracks (value  between 0.0 and 1.0, with 1.0 being the most energetic); represents perceptual measure of intensity and activity</td>
      </tr>
      <tr>
        <td>majority_key</td>
        <td>key of the majority of component tracks using standard Pitch Class notation (0 = C, 1 = C#/Db, 2 = D, etc.)</td>
      </tr>
      <tr>
        <td>majority_time_signature</td>
        <td>time signature of majority of component tracks</td>
      </tr>
    </tbody>
  </table>

  <h2><b>Literature Review/Related Work</b></h2>
  <p>Our data came from both Spotify and the Million Song Dataset. We read about the Million Song Dataset Challenge on Kaggle and obtained our data from there. We then used the file <a href="https://github.com/michelle-chiang/CS109-Final-Project/blob/master/MSD/Million-Song-Dataset-HDF5-to-CSV-master/msdHDF5toCSV.py">msdHDF5toCSV.py</a> to aggregate the data from the MSD in HDF5 format to generate a CSV with features to add to our dataframe for further analysis. The Million Song Dataset data is attributed to Thierry Bertin-Mahieux (2010) at Columbia University and Alexis Greenstreet (2015) University of Wisconsin-Madison.</p>

</div>
</body>
</html>